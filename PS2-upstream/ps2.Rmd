---
title: "Problem Set 2"
author: Gaurav Khanna
output:
  pdf_document: default
---


```{r setup}
library(data.table)
library(dplyr)
```

```{r}
library(experiment)
```
<!--
Some guidelines for submitting problem sets in this course:

- Please submit a PDF document rather than a Word document or a Google document.
- Please put your name at the top of your problem set.
- Please **bold** or *highlight* your numerical answers to make them easier to find.
- If you'll be using `R` or `Python` code to calculate your answers, please put the code and its output directly into your Problem Set PDF document.
- It is highly recommended, although not required, that you use the RMarkdown feature in RStudio to compose your problem set answers. RMarkdown allows you to easily intermingle analysis code and answers in one document. It is of a similar design as `jupyter` and an ipython notebook.
- You do not need to show work for trivial calculations, but showing work is always allowed.
- For answers that involve a narrative response, please feel free to describe the key concept directly and briefly, if you can do so, and do not feel pressure to go on at length.
- Please ask us questions about the problem set if you get stuck. **Don’t spend more than 20 minutes puzzling over what a problem means.** 
- Please ensure that someone (us!) can compile your solution set. The best way is to use the web-hosted links we've provided. 
--> 

# 1. What happens when pilgrims attend the Hajj pilgrimage to Mecca? 

On the one hand, participating in a common task with a diverse group of pilgrims might lead to increased mutual regard through processes identified in *Contact Theories*. On the other hand, media narritives have raised the spectre that this might be accompanied by "antipathy toward non-Muslims". [Clingingsmith, Khwaja and Kremer (2009)](https://dash.harvard.edu/handle/1/3659699) investigates the question. 

Using the data here, test the sharp null hypothesis that winning the visa lottery for the pilgrimage to Mecca had no effect on the views of Pakistani Muslims toward people from other countries. Assume that the Pakistani authorities assigned visas using complete random assignment. Use, as your primary outcome the `views` variable, and as your treatment feature `success`. If you're ambitious, write your fucntion generally so that you can also evaluate feeligns toward specific nationalities.

```{r}
# Dataframe from the CSV
d <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)

```
a. Using either `dplyr` or `data.table`, group the data by `success` and report whether views toward others are generally more positive among lottery winners or lottery non-winners. 
```{r}
head(d)
```

```{r}
# Grouping data by success
# success <-group_by(d, success)
# TODO: Chaining the operations is more elegant but i'd like to see the intermediate
success <- group_by(d, success)
summarize(success, count=n(), viewSum=sum(views), viewMean=mean(views))
```
**Answer: The mean of views among lottery winners and losers is 2.34 and 1.86 respcetively**. The **views for others generally look more positive among the lottery winners**

```{r}
# Estimate ATE function
est.ate <- function(result, treat) { 
  mean(result[treat==1]) - mean(result[treat==0])
  } 
```
```{r}
ate <- est.ate(success$views, success$success) 
ate
```
** Answer: A positive ATE 0.4748337 adds more weight to above statement**

b. But is this a meaningful difference, or could it just be randomization noise? Conduct 10,000 simulated random assignments under the sharp null hypothesis to find out. (Don't just copy the code from the async, think about how to write this yourself.) 

```{r}
# Sharp null allows us to complete the data, y(0) and y(1) from the data we already have
# We just need to randomize the success variable
# Creating a new data frame to try randomization on the orgiginal data
# The original is kept around intact for reference
dr <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)
head(dr)
success1 <- group_by(dr, success)
summarize(success1, count=n(), viewSum=sum(views), viewMean=mean(views))

```

```{r}
# Randomizing the success variable
# randomize() from the expriments library, returns the "randomize" class
# randomize() creates 2 groups treatment and control which is sufficient for us in this case
# It is equivalent to us randomizing among 0 and 1 for "success" variable
# We are not doing any blocking, so subjects in a group can be variable. We are choosing to keep them as close to equal
successr <- randomize(success1)
```


```{r}
# Some statistics on the classes after randomization
# We're choosing to pick equal numbers in the 2 classes
head(successr$treatment)
table(success1$success)
table(successr$treatment)
```

```{r}
# Modifying est.ate to use the object of "randomize" class
est.mate <- function(result, treat) { 
  mean(result[treat=="Treat"]) - mean(result[treat=="Control"])
  } 
```


```{r}
# ATE for 1 randomization for testing
ateR <- est.mate(successr$data$views, successr$treatment)
ateR
# This is very different from our ATE
```
```{r}
# Repeating the expriment 10,000 times
distribution.under.sharp.null <- replicate(10000, est.mate(successr$data$views, (randomize(success1))$treatment))

# Average estimate for the ATE
mean(distribution.under.sharp.null)
# Again, very different from ATE
```
```{r}
# Graph for the estimates
plot(density(distribution.under.sharp.null), 
     main = "Density under Sharp Null")
# Almost a normal distribution
```
```{r}
# Histogram for the distribution
hist(distribution.under.sharp.null, 
     main = "Histogram under Sharp Null")
```


```{r}
# More analysis on the distribution. Plotting the p value
par(mfrow = c(1,2))
plot(density(distribution.under.sharp.null), 
     main = "Density Plot of ATE")
abline(v = ate, col = "blue")
hist(distribution.under.sharp.null, 
     main = "Histogram of ATE", 
     freq = FALSE)
abline(v = ate, col = "blue")
```
**Answer: Randomization done above. Graphs show the normal distribution of ATE estimates**

c. How many of the simulated random assignments generate an estimated ATE that is at least as large as the actual estimate of the ATE? 

```{r}
# ate was
ate

# num of assignments that generate an estimated ATE at least as large as the actual
n <- sum(distribution.under.sharp.null >= ate)
n
```
**Answer: 25 assignments generate an estimated ATE at least as large as the actual**

d. What is the implied *one-tailed* p-value? 

```{r}
# p-value
m <- mean(distribution.under.sharp.null >= ate )
m

```
**Answer: One tailed p-value is .0025**

e. How many of the simulated random assignments generate an estimated ATE that is at least as large *in absolute value* as the actual estimate of the ATE? 

```{r}
# num of simulated random assignments with an estimated ATE that is at least as large *in absolute value* as the actual estimate of the ATE
na <- sum(abs(distribution.under.sharp.null) >= abs(ate))
na

```
**Answer: 42 simulated random assignments generate an estimated ATE that is at least as large "in absolute value" as the actual estimate of the ATE**

f. What is the implied two-tailed p-value? 

```{r}
# 2 tailed p-value
ma <- mean(abs(distribution.under.sharp.null) >= ate )
ma
```
**Answer: The 2 tailed p-value is .0042**
** The p value is less than .05. We should be able to reject the SHARP NULL hypothesis of No treatment effect**

**_____________________________________________**

# 2. Term Limits Aren't Good. 

Naturally occurring experiments sometimes involve what is, in effect, block random assignment. For example, [Rocio Titiunik](https://sites.google.com/a/umich.edu/titiunik/publications) , in [this paper](http://www-personal.umich.edu/~titiunik/papers/Titiunik2016-PSRM.pdf) studies the effect of lotteries that determine whether state senators in TX and AR serve two-year or four-year terms in the aftermath of decennial redistricting. These lotteries are conducted within each state, and so there are effectively two distinct experiments on the effects of term length.

The "thoery" in the news (such as it is), is that legislators who serve 4 year terms have more time to slack off and not produce legislation. If this were true, then it would stand to reason that making terms shorter would increase legislative production. 

One way to measure legislative production is to count the number of bills (legislative proposals) that each senator introduces during a legislative session. The table below lists the number of bills introduced by senators in both states during 2003. 

```{r}
library(foreign)

d2 <- read.dta("./data/Titiunik.2010.dta")
head(d2)
```

a. Using either `dplyr` or `data.table`, group the data by state and report the mean number of bills introduced in each state. Does Texas or Arkansas seem to be more productive? Then, group by two- or four-year terms (ignoring states). Do two- or four-year terms seem to be more productive? **Which of these effects is causal, and which is not?** Finally, using `dplyr` or `data.table` to group by state and term-length. How, if at all, does this change what you learn? 

```{r}
# Grouping the data by state
state <- group_by(d2, texas0_arkansas1)
head(state)

# Mean number of bills introduced in each state
summarize(state, count=n(), meanBills=mean(bills_introduced))

# Which state looks more productive? 

```
**Answer: Texas has a mean of 68.77 compared to 25.51 for Arkansas. Texas's legislative sessions seem to produce more bills in average/session**
**There is significant difference (significant as in value on average) among the legislative performance that one has to wonder about state/location being causal. A Question would be that if a Senator starts producing more bill when she moves from Arkansas to Texas?**
** The answer to this Q (and building a counterfactual argumet) does not seem very viable or even interesting, so we'll not investigate state as causal**

```{r}
# Grouping by terms
terms <- group_by(d2, term2year)
head(terms)

# Mean number of bills by terms
summarize(terms, meanBills=mean(bills_introduced))

# Which term is more effective? 

```
**Answer: 4 year terms seem to produce more bills (53.09) per legislative session on an average, when compared to average number of bills for a 2 year term (38.57)**
**Again, a significant difference in performance among the 2 groups. Motivates us to look at cuasality**
**We can change the term limits and observe performance. In a way, that's what the experiment does within each state. Terms definately could be causal and we should be able to measure the effect with experiments**

Which of this effect is causal? 
**Answer: It looks more intersting to investigate "Term2year" as Causal**

```{r}
# Grouping by state and term term
stateTerms <- group_by(d2, texas0_arkansas1, term2year)
head(stateTerms)

# Mean number of bills by grouping
summarize(stateTerms, meanBills=mean(bills_introduced))

# Which groupings indicate more effectiveness? 


```
**Answer: 2 year terms on an average produce less bills(in a session) in Texas**
**Same is the case in Arkansas**
**So within a state, different terms show different legislative output**

**This should motivate us to look at the lenght of term (term2year) as causal**

b. For each state, estimate the standard error of the estimated ATE. 

```{r}
# Separating out the states

# Texas
texas <- state[which(state$texas0_arkansas1==0),]
head(texas)

# Finding out the control/treatment ratio
texas.group <- group_by(texas, term2year)
summarize(texas.group, count=n())

# Arkansas
arkansas <- state[which(state$texas0_arkansas1==1),]
head(arkansas)

# Control/Treatment ratio
arkansas.group <- group_by(arkansas, term2year)
summarize(arkansas.group, count=n())

```

```{r}
# ATE for Texas
texas_ate <- est.ate(texas$bills_introduced, texas$term2year)
texas_ate

# Considering the distribution with other random assignments
# We'll randomize the term variable. "2 years" is our treatment
# Since the grouping is binary, randomization of the entire dataset should work
# Also the groups are almost of the same size. We can work with that assumption

# Ratio of treatment and control
rio.texas <- c(sum(texas$term2year), nrow(texas) - sum(texas$term2year))
rio.texas

# We'll create random sets in the same ratio
texas_r <- randomize(texas)


```


```{r}
# Repeating the expriment 10,000 times
distribution.under.sharp.null.texas <- replicate(10000, est.mate(texas_r$data$bills_introduced, (randomize(texas))$treatment))
```
```{r}
# Visualizing the distribution of ATE estimates
plot(density(distribution.under.sharp.null.texas), 
     main = "Density under Sharp Null for Texas")

```
```{r}
# Histogram of the ATE estimates
hist(distribution.under.sharp.null.texas, 
     main = "Histogram under Sharp Null for Texas")
```


```{r}
# average estimated ate for texas
# Debug: mean(distribution.under.sharp.null.texas)

# Standard error is the square root of the averaged squared deviation (from the average EST estimate)
# Debug head(distribution.under.sharp.null.texas)
se_texas <- sqrt(mean((distribution.under.sharp.null.texas-mean(distribution.under.sharp.null.texas))^2))
se_texas



```
**Answer: The standard error for ATE estimates in Texas is 9.867**
```{r}
# ATE for arkansas
arkansas_ate <- est.ate(arkansas$bills_introduced, arkansas$term2year)
arkansas_ate

# Considering the distribution with other random assignments
# We'll randomize the term variable. "2 years" is our treatment
# Since the grouping is binary, randomization of the entire dataset should work
arkansas_r <- randomize(arkansas)


```

```{r}
# Repeating the expriment 10,000 times
distribution.under.sharp.null.arkansas <- replicate(10000, est.mate(arkansas_r$data$bills_introduced, (randomize(arkansas))$treatment))
```
```{r}
plot(density(distribution.under.sharp.null.arkansas), 
     main = "Density under Sharp Null for arkansas")
```
```{r}
hist(distribution.under.sharp.null.arkansas, 
     main = "Histogram under Sharp Null for Arkansas")
```

```{r}
# average estimated ate for arkansas
# mean(distribution.under.sharp.null.arkansas)

# Standard error is the square root of the averaged squared deviation (from the average EST estimate)
# head(distribution.under.sharp.null.arkansas)
se_arkansas <- sqrt(mean((distribution.under.sharp.null.arkansas-mean(distribution.under.sharp.null.arkansas))^2))
se_arkansas
```
**Answer: The standard error for ATE estimate for arkansas is 3.77561**

c. Use equation (3.10) to estimate the overall ATE for both states combined. 

```{r}
# ATE for both sides combined
# Recalling previous values and then using 3.10

# ATE for texas from the experiment
texas_ate
# ATE for arkansas from the experiment
arkansas_ate
# Number of observations in texas
nrow(texas)
# Number of observations in Arkansas
nrow(arkansas)
# Total number of observations
nrow(d2)

# Combined ATE
comb_ate <- (texas_ate*nrow(texas)/nrow(d2)) + (arkansas_ate*nrow(arkansas)/nrow(d2))
comb_ate


```
**Answer: The overall ATE is -13.2168**

d. Explain why, in this study, simply pooling the data for the two states and comparing the average number of bills introduced by two-year senators to the average number of bills introduced by four-year senators leads to biased estimate of the overall ATE. 

```{r}
# The summary below reveals a lof of why pooling this data would produce a biased estimate of ATE. We see that in actual calculations later

# Mean number of bills by grouping
summarize(stateTerms, meanBills=mean(bills_introduced))

```

```{r}
# What if we pooled data for both states to calculate the ATE (Instead of the method above)?
# Value of ATE from the experiment with pooled data
pooled.ate <- est.ate(d2$bills_introduced, d2$term2year)
pooled.ate
```
**Its quite similar to the value we get from the calculation of the overall ATE**
**Answer: The pooled data produces a biased estimate of ATE due to the different baselines among the 2 states. Texas average is 76.8 for control, while Arkansas is at 30.70, which is less than half the baseline for Texas.**

e. Insert the estimated standard errors into equation (3.12) to estimate the stand error for the overall ATE. 

```{r}
# Recalling previous values and then putting them in 3.12 for combined SE

# Standard ATE error for Texas
se_texas
# Standard ATE error for Arkansas
se_arkansas

# Standar error for the 2 combined
se_comb <- sqrt((nrow(texas)*se_texas/nrow(d2))^2 + (nrow(arkansas)*se_arkansas/nrow(d2))^2)
se_comb

```
**Answer: Standard Error for the overall ATE is 5.050909**

f. Use randomization inference to test the sharp null hypothesis that the treatment effect is zero for senators in both states. 

```{r}
# Randomization inference to test the sharp null hypothesis (treatment effect is 0 in Texas)

# We have the results of 10,000 randomizations above in distribution.under.sharp.null.texas
head(distribution.under.sharp.null.texas)
# We also have the ate from the actual expriment
texas_ate

# Two tailed p value
p_two_texas <- mean(abs(distribution.under.sharp.null.texas) >= abs(texas_ate))
p_two_texas
``` 
**Answer: p value for ATE for Texas is 0.0893**
**p value is close but not less than .05. We cannot reject the SHARP NULL hypothesis**
```{r}
# Randomization inference to test the sharp null hypothesis (treatment effect is 0 in Arkansas)

# We have the results of 10,000 randomizations above in distribution.under.sharp.null.texas
head(distribution.under.sharp.null.arkansas)
# We also have the ate from the actual expriment
arkansas_ate

# Two tailed p value
p_two_arkansas <- mean(abs(distribution.under.sharp.null.arkansas) >= abs(arkansas_ate))
p_two_arkansas
```
**Answer: p value for ATE for Arkansas is 0.0049**
**p value is less than .05. We can reject the SHARP NULL hypothesis**

g. **IN Addition:** Plot histograms for both the treatment and control groups in each state (for 4 histograms in total).
**Answer: Tough to compare with 2 different histograms. Trying to overlap**
```{r}
# Histogram for control and Treatment in Texas
hist(texas[which(state$term2year == 0),]$bills_introduced, xlim = c(0,140), ylim = c(0,6), col = "blue", border = F, main = "Distribution of bills_introduced under control and treatment in Texas", xlab = "Number of bills introduced in a session", density=10, angle = 135)
hist(texas[which(state$term2year == 1),]$bills_introduced, xlim = c(0, 140), ylim = c(0,6), col= "green", add = T, density = 10, angle = 45)
legend('topleft',c('Control','Treatment'),
       fill = c("blue", "green"), bty = 'n',
       border = NA)

```

```{r}
# Histogram for control and treatment in Arkansas
hist(arkansas[which(state$term2year == 0),]$bills_introduced, xlim = c(0,70), ylim = c(0,10), col = "blue", border = F, main = "Distribution of bills_introduced under control and treatment in Arkansas", xlab = "Number of bills introduced in a session", density=10, angle = 135)
hist(arkansas[which(state$term2year == 1),]$bills_introduced, xlim = c(0, 70), ylim = c(0,10), col= "green", add = T, density = 10, angle = 45)
legend('topright',c('Control','Treatment'),
       fill = c("blue", "green"), bty = 'n',
       border = NA)
```

**____________________________________________**

# 3. Cluster Randomization
Use the data in *Field Experiments* Table 3.3 to simulate cluster randomized assignment. (*Notes: (a) Assume 3 clusters in treatment and 4 in control; and (b) When Gerber and Green say ``simulate'', they do not mean ``run simulations with R code'', but rather, in a casual sense ``take a look at what happens if you do this this way.'' There is no randomization inference necessary to complete this problem.*)


```{r}
## load data 
d3 <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
```

a. Suppose the clusters are formed by grouping observations {1,2}, {3,4}, {5,6}, ... , {13,14}. Use equation (3.22) to calculate the standard error assuming half of the clusters are randomly assigned to treatment. 

```{r} 
# Add a cluster ID to data
d3$cluster <- NA
# Assign the clusters as stated in "a"
d3$cluster <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7)
head(d3)
```

```{r}
# pull out the cluster ID's
all_clusters <- unique(d3$cluster)
all_clusters
# Number of clusters in treatment
clusters.in.treatment <- 3
# Generic function to randomly(pseudo) pick the cluster ID's
randomize.clusters <- function(d) {
  treat.clusters <- sample(x = all_clusters, 
                                size = clusters.in.treatment, 
                                replace = FALSE)
  # returns 1 if a cluster ID in those that are picked
  return(as.numeric(d$cluster %in% treat.clusters))
}

# Trying out one randomization. Now we have the data prepared for the experiment
d3$treatment <- randomize.clusters(d3)

# QUESTION & TODO More randomizations to get an estimate. For now just treating it as one expriment

# Grouping data by clusters and adding the treatment column(treatment-1, control-0 for the cluster)
clusters <- group_by(d3, cluster)
clusters.summary <- summarize(clusters, count=n(), mean_y=mean(Y), mean_d=mean(D), treatment=sum(treatment)/2 )
clusters.summary

```

```{r}

# QUESTION When calculating variances, are we calculating among all the clusters or clusters in control and treatment respectively? My assumption is that we are doing it among all the clusters, just like in the book where the example (page 56) shows the Formulaes that take into account the entire N

# Variance in y(0) clustered
variance_y <- (sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))^2))/nrow(clusters.summary)
variance_y

# Variance in y(1) clustered
variance_d <- (sum((clusters.summary$mean_d - mean(clusters.summary$mean_d))^2))/nrow(clusters.summary)
variance_d

# Covariance among y(0) and Y(1)
covariance <- sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))*(clusters.summary$mean_d - mean(clusters.summary$mean_d)))/nrow(clusters.summary)
covariance
```
```{r}
# applying 3.22

# m is subjects in treatment (not clusters). N is count of subjects, number of clusters is 7
m <- clusters.in.treatment * 2
m
N <- nrow(d3)
N
nrow(clusters.summary)

se <- sqrt(((m*variance_y/N-m) + ((N-m)*variance_d/m) + 2*covariance)/(nrow(clusters.summary) - 1))
se

```
**Answer: The standard error for one randomization (3 groups in treatment) is 4.196791 **

b. Suppose that clusters are instead formed by grouping observations {1,14}, {2,13}, {3,12}, ... , {7,8}. Use equation (3.22) to calculate the standard error assuming half of the clusters are randomly assigned to treatment. 

```{r} 
# repeating by grouping the observations in a different way
d4 <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
# Add the cluster ID to data
d4$cluster <- NA
# Assign the clusters for "a"
d4$cluster <- c(1,2,3,4,5,6,7,7,6,5,4,3,2,1)
head(d4)
# Trying out one randomization. Now we have the data prepared for the experiment
d4$treatment <- randomize.clusters(d4)
# Grouping by the cluster and creating the summary
clusters <- group_by(d4, cluster)
clusters.summary <- summarize(clusters, count=n(), mean_y=mean(Y), mean_d=mean(D), treatment=sum(treatment)/2 )
clusters.summary
# Variance in y(0) clustered
variance_y <- (sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))^2))/nrow(clusters.summary)
variance_y

# Variance in y(1) clustered
variance_d <- (sum((clusters.summary$mean_d - mean(clusters.summary$mean_d))^2))/nrow(clusters.summary)
variance_d

# Covariance among y(0) and Y(1)
covariance <- sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))*(clusters.summary$mean_d - mean(clusters.summary$mean_d)))/nrow(clusters.summary - 1)
covariance

# applying 3.22

# m is subjects in treatment (not clusters). N is count of subjects, number of clusters is 7
m <- clusters.in.treatment * 2
m
N <- nrow(d4)
N
nrow(clusters.summary)

se1 <- sqrt(((m*variance_y/N-m) + ((N-m)*variance_d/m) + 2*covariance)/(nrow(clusters.summary) - 1))
se1
``` 
**Answer: The standard error for one randomization (3 groups in treatment) is 0.5814735**

c. Why do the two methods of forming clusters lead to different standard errors? What are the implications for the design of cluster randomized experiments? 

**Answer: The methods lead to different standard errors due to differences in the cluster means (for outcomes in control and treatment) which indicate the similarities or differences among the subjects (villages in this case, that become part of a cluster).**
**In this problem, the first method was akin to clustering based on geography. Villages next to each other (in Sr. no), had similar outcomes and ended up in the same cluster, leading to large differences among the clusters. Method 2 mixed things up and clustered villages that had "more" different outcomes. This sharply reduced the differences among the clusters**

**Answer: The implication is that cluster design has to account for averages of outcomes among the subjects, or we'll get imprecise estimates for the treatment effect.**
The **best scenario would be to bring down the variance among clusters by grouping unlike subjects** (like method 2 where dissimilar villages were clustered together). This may not be possible as clustering is mostly forced (like villages, schools by geography).
**The next best idea could be to increase the number of clusters and then simulate all possible random assignments of clusters** to obtain the sampling distribution **

**_______________________________________** 

# 4. Sell Phones? 
You are an employee of a newspaper and are planning an experiment to demonstrate to Apple that online advertising on your website causes people to buy iPhones. Each site visitor shown the ad campaign is exposed to $0.10 worth of advertising for iPhones. (Assume all users could see ads.) There are 1,000,000 users available to be shown ads on your newspaper’s website during the one week campaign. 

Apple indicates that they make a profit of $100 every time an iPhone sells and that 0.5% of visitors to your newspaper’s website buy an iPhone in a given week in general, in the absence of any advertising.

a. By how much does the ad campaign need to increase the probability of purchase in order to be “worth it” and a positive ROI (supposing there are no long-run effects and all the effects are measured within that week)?

```{r}
# Each site visitor shown the ad is exposed to $.10 of ads
# There are 1,000,000 users in one week
# Profit is $100 on every phone
# y(0) - .5% of visitors to news site buy an iphone in a week (in absence of ads)

# Users for the website in a wee
week.users <- 1000000
week.users
# Profit per phone sold
profit_per_phone <- 100
profit_per_phone
# % of visitors that buy without ads. This can be considered "control"
week.percent_users_buy <- .5
week.percent_users_buy
# Number of users that buy (irrespective of the ads)
week.number_users_buy_gen <- (.5/100)*1000000
week.number_users_buy_gen
# Total profit in a week (irrespective of the ads)
week.profit_gen <- week.number_users_buy_gen * profit_per_phone
week.profit_gen
# ad spend per user and per week for all users. This assumes all users are exposed to the ads
adspend_per_user <- .10
adspend_per_week <- adspend_per_user * week.users
adspend_per_week
# We should atleast make up the money spend on ads in a week. That means we have to sell +inc number of phones which is equivalent to +inc units of profit
inc <- adspend_per_week/profit_per_phone
inc
# so we have to sell 1000 additional phones
# Which is obs % points based on the original population eligible for ads and purchases
inc_per <- (inc/week.users)*100
inc_per
# This is breakeven effect of the advertising. Anything more would be positive ROI
# Also, we assumed that all users were shown the ad so 

# Say we show the ads to only half the population. The ad spend would be half, but then we'll measure the effect on half the population also. The %'s would remain the same.



```
**Answer: The ads would have to convert an additional .1% website user to buyers**
**This'll sell 1000 additional phones, which creates 1e5 of incremental profits, which covers the (1e-1)(1e6) cost of advertising**

b. Assume the measured effect is 0.2 percentage points. If users are split 50:50 between the treatment group (exposed to iPhone ads) and control group (exposed to unrelated advertising or nothing; something you can assume has no effect), what will be the confidence interval of your estimate on whether people purchase the phone?

```{r}
# Measured effect is .2% points
inc_measured <- .2
inc_measured

# users in control and treatment
week.users.control <- week.users/2
week.users.treatment <- week.users/2
week.users.control
week.users.treatment

# p by the formula in the question
p <- (week.users.control*week.percent_users_buy/100 + week.users.treatment*(week.percent_users_buy+inc_measured)/100)/(week.users.control + week.users.treatment)
p

# se by the formula in the question
se <- sqrt(p*(1-p)*((1/week.users.control) + (1/week.users.treatment)))
se

# tail of a 95% confidence interval
ci_tail <- se*1.96
ci_tail

# Upper and Lower limits of the confidence interval
ci1 <- .002 + ci_tail
ci1
ci2 <- .002 - ci_tail
ci2

# Width of the confidence interval
ci.wid1 <- ci1 - ci2
ci.wid1

# % size of the CI
(ci.wid1/(inc_measured/100))*100
```
**Answer: The 95% confidence interval would be [0.00169727, 0.00230273]**

  + **Note:** The standard error for a two-sample proportion test is $\sqrt{p(1-p)*(\frac{1}{n_{1}}+\frac{1}{n_{2}})}$ where $p=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}$, where $x$ and $n$ refer to the number of “successes” (here, purchases) over the number of “trials” (here, site visits). The length of each tail of a 95% confidence interval is calculated by multiplying the standard error by 1.96.
  
c. Is this confidence interval precise enough that you would recommend running this experiment? Why or why not?
**Answer: The 95% confidence level calculated above meets our needs. Since profitability is a concern, its reassuing that the lower bound is above the customer conversion rate(visitor buying phones on seeing the AD) for positive ROI (Our campaign has positive ROI above .1% ad conversion rate)**
**Its also reassuring that the interval is narrow but includes both the "minimum requirment for ROI" (.1%) and the observed value of the expriment(.2%)**

d. Your boss at the newspaper, worried about potential loss of revenue, says he is not willing to hold back a control group any larger than 1% of users. What would be the width of the confidence interval for this experiment if only 1% of users were placed in the control group?

```{r}
# what if only 1% of users were in control

# users in control and treatment
week.users.control <- .01*week.users
week.users.treatment <- week.users - week.users.control
week.users.control
week.users.treatment

# p by the formula in the question
p <- (week.users.control*week.percent_users_buy/100 + week.users.treatment*(week.percent_users_buy+inc_measured)/100)/(week.users.control + week.users.treatment)
p
# se by the formula in the question
se <- sqrt(p*(1-p)*((1/week.users.control) + (1/week.users.treatment)))
se

# tail of a 95% confidence interval
ci_tail <- se*1.96
ci_tail

# Upper and Lower limits of the confidence interval
ci1 <- .002 + ci_tail
ci1
ci2 <- .002 - ci_tail
ci2

# Width of the confidence interval
ci.wid2 <- ci1 - ci2
ci.wid2

```
**Answer: The 95% confidence interval with 1% in control is would be [0.000359995, 0.003640005]**
**Width of the confidence interval is 0.00328001  which is ~5 times the width of the confidence interval with 50% of the sample in control**



# 5. Sports Cards
Here you will find a set of data from an auction experiment by John List and David Lucking-Reiley ([2000](https://drive.google.com/file/d/0BxwM1dZBYvxBNThsWmFsY1AyNEE/view?usp=sharing)).  

```{r}
d5 <- read.csv("./data/listData.csv", stringsAsFactors = FALSE)
head(d5)
```

In this experiment, the experimenters invited consumers at a sports card trading show to bid against one other bidder for a pair trading cards.  We abstract from the multi-unit-auction details here, and simply state that the treatment auction format was theoretically predicted to produce lower bids than the control auction format.  We provide you a relevant subset of data from the experiment.

a. Compute a 95% confidence interval for the difference between the treatment mean and the control mean, using analytic formulas for a two-sample t-test from your earlier statistics course. 

```{r}
# 95% confidence interval for the difference between the treatment mean and the control mean

# Grouping the data by treatment (& control)
by.treatment <- group_by(d5, uniform_price_auction)
# Summary
by.treatment.summary <- summarize(by.treatment, count=n(), mean.bid=mean(bid), sd.bid=sd(bid))
by.treatment.summary

```

```{r}
# Seems we have 2 equally sized groups (control and treatment) with very different average bid sizes but not a very sharp difference in standard deviation

# Getting an idea of normality (an essential for the T test)

# Extract the control and treatment separately
bid.control <- d5[which(d5$uniform_price_auction == 0),]
nrow(bid.control)
head(bid.control)
bid.treatment <- d5[which(d5$uniform_price_auction == 1),]
nrow(bid.treatment)
head(bid.treatment)

# plots
plot(density(bid.control$bid))
plot(density(bid.treatment$bid))
```

**Visual test for normality (essential for the T test): The distribution of bids in control is close to normal. Bids in treatment have a bias.**

```{r}
# T test using the analytic formula

# Mean of the 2 samples (control and test)
bid.control.mean <- mean(bid.control$bid)
bid.control.mean
bid.treatment.mean <- mean(bid.treatment$bid)
bid.treatment.mean

# Difference of the means
stat.mean.diff <- bid.treatment.mean - bid.control.mean
stat.mean.diff

# degrees of freedom
stat.df <- nrow(bid.control) + nrow(bid.treatment) -2
stat.df

# sample variances
stat.control.variance <- sum((bid.control$bid - bid.control.mean)^2)/nrow(bid.control)
stat.control.variance
stat.treatment.variance <- sum((bid.treatment$bid - bid.treatment.mean)^2)/nrow(bid.treatment)
stat.treatment.variance

# Pooled standard deviation
sdpool.small <- sqrt(((nrow(bid.control)-1)*stat.control.variance + (nrow(bid.treatment)-1)*stat.treatment.variance)/stat.df)
sdpool.small

# Pooled standard deviation with formula used for sample sizes > 30
sdpool.large <-sqrt((stat.control.variance/nrow(bid.control)) + (stat.treatment.variance/nrow(bid.treatment)))
sdpool.large

# SSQ
ssq <- (sum((bid.control$bid - bid.control.mean)^2)+ sum((bid.treatment$bid - bid.treatment.mean)^2))/(nrow(bid.control) + nrow(bid.treatment) -2)
ssq
sd <- sqrt(ssq)
sd

# We'll use the small sample 34 is very close to the "30"
# Boundaries of the confidence interval
# Taking the t value (t.75) from the tables
ci1 <- stat.mean.diff + 2.00 * sdpool.small * (sqrt(1/nrow(bid.control) + 1/nrow(bid.treatment)))
ci1
ci2 <- stat.mean.diff - 2.00 * sdpool.small * (sqrt(1/nrow(bid.control) + 1/nrow(bid.treatment)))
ci2

# t statistic
stat.t <- (bid.treatment.mean - bid.control.mean)/sqrt((ssq/nrow(bid.treatment)) + (ssq/nrow(bid.control)))
stat.t

# Verified with the R calculations below
```
**The 95% confidence interval from the calculations is: [-20.73082, -3.68094]**

```{r}
# T test using R to verify the calculations
res <- t.test(bid.treatment$bid, bid.control$bid, var.equal = TRUE)
res
```
**p value is .0063, which is less than the significance level alpha = .05**
**95% confidence interval is [-20.844162, -3.567603]**

b. In plain language, what does this confidence interval mean?
**The confidence interval [-20.844162, -3.567603] means that if we do a large number of measurements, there's a 95% chance that the difference in means would be in the range [-20.844162, -3.567603]**
**This makes us reject the NULL hypothesis that the means are the same**

c. Regression on a binary treatment variable turns out to give one the same answer as the standard analytic formula you just used.  Demonstrate this by regressing the bid on a binary variable equal to 0 for the control auction and 1 for the treatment auction.

```{r}
# Regression
reg <- lm(bid ~ uniform_price_auction,
                 data=d5)
summary(reg)

```
**We get exactly the same results**
**p value ( 0.006315 )**
**The equation is bid = 28.824 -12.206(uniform_price_auction)**

d. Calculate the 95% confidence interval you get from the regression.

```{r}
# The 95% confidence interval is the slope (coeff for "uniform_price_auction") +- 2 standard errors
# Could also be calculated using confint
confint(reg, 'uniform_price_auction', level=.95)
```
**The 95% confidence interval for the coeff of "uniform_price_auction" is the coeff +- 2 standard errors(.4327 from the summary)**
**We get the same interval as the T test**
**                          2.5 %    97.5 % **
**-uniform_price_auction -20.84416 -3.567603**


e. On to p-values. What p-value does the regression report? Note: please use two-tailed tests for the entire problem.

```{r}
# Coefficients and p values from the regression
summary(reg)

```
**The regression reports a p value (for 2 sided hypotheses): 0.006315**

f. Now compute the same p-value using randomization inference

```{r}
# Some analysis of the data
bids.group <- group_by(d5, uniform_price_auction)
head(bids.group)
bids.summary <- summarize(bids.group, count=n())
bids.summary
```

```{r}
# p value using the randomization inference

# ATE observed
# Using the ATE formula from a
bids.ate <- est.ate(d5$bid, d5$uniform_price_auction)
bids.ate

# Randomizing the bids (among control and treatment). 
# uniform_price_auction is binary so we can just assume 0, 1 as control and treatment
# We'll keep the ratio control:treatment the same as observed (34:34)
bids.random <- randomize(d5)

# DEBUG: For curiosity figuring out the est ATE of 2 of the randomization done above
# Using the modified ATE formula done for Q a
bids.rate <- est.mate(bids.random$data$bid, bids.random$treatment)
bids.rate

bids.random <- randomize(d5)
bids.rate <- est.mate(bids.random$data$bid, bids.random$treatment)
bids.rate

# So we get quite a different value from the actual and  quite different values each time

```
```{r}
# Repeating the experiment 5k times and getting the distribution under sharp null
# We are most interested in the differences in mean statistic
dom.distribution.under.sharp.null <- replicate(5000, est.mate(bids.random$data$bid, (randomize(d5))$treatment))


# Average estimate for the ATE
mean(dom.distribution.under.sharp.null)
# Again, very different from ATE

```

```{r}
# visualizing the distribution
plot(density(dom.distribution.under.sharp.null), main = "difference of means distribution under sharp NULL")
hist(dom.distribution.under.sharp.null, main = "differences of means distribution under sharp NULL")
```
```{r}
# More analysis on the distribution. Plotting the p value
par(mfrow = c(1,2))
plot(density(dom.distribution.under.sharp.null), 
     main = "Density Plot of ATE")
abline(v = bids.ate, col = "blue")
hist(dom.distribution.under.sharp.null, 
     main = "Histogram of ATE", 
     freq = FALSE)
abline(v = bids.ate, col = "blue")
```



```{r}
bids.ate 
# observations with absolute value >= ate
sum(abs(dom.distribution.under.sharp.null) >= abs(bids.ate))
# 2 tailed p-value
p2 <- mean(abs(dom.distribution.under.sharp.null) >= abs(bids.ate))
p2

# observations with value >= ate
sum(dom.distribution.under.sharp.null >= abs(bids.ate))
# single tail p value
p1 <- mean(dom.distribution.under.sharp.null >= abs(bids.ate))
p1
```
**The 2 sided p value from randomization inference is .0062**

g. Compute the same p-value again using analytic formulas for a two-sample t-test from your earlier statistics course. (Also see part (a).)

```{r}
# p value using the analytic formulas
# p value is the area of the t distribution > t statistic (calculated above)
# We double it to get to the 2 sided value
# We'll look up a table to confirm

# Value from the table corresponding to t statistic of -2.8211 and degrees of freedom = 66
table.p1 <- .003159
table.p2 <- table.p1 * 2
table.p2


```
**The p value from the t test is 0.006318**

h. Compare the two p-values in parts (e) and (f). Are they much different? Why or why not? How might your answer to this question change if the sample size were different?

**p value from (e)(Regression): 0.006315**
**p value from (f)(Randomization inference): 1**

