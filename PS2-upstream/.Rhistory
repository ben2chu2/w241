# The original is kept around intact for reference
dr <- read.csv("./data/Clingingsmith.2009.csv", stringsAsFactors = FALSE)
head(dr)
success1 <- group_by(dr, success)
success.summary <- summarize(success1, count=n(), viewSum=sum(views), viewMean=mean(views))
success.summary
g.randomize <- function(control, treatment) {
sample(c(rep(0, control), rep(1, treatment)))
}
# Randomizing the success variable
# We are not doing any blocking, so subjects in a group can be variable
# We are choosing to keep the ratios the same as the actual experiment
count.control <- success.summary[which(success.summary$success == 0),]$count
count.control
count.treatment <- success.summary[which(success.summary$success == 1),]$count
count.treatment
successr <- g.randomize(count.control, count.treatment)
head(successr)
# Some statistics on the classes after randomization
head(successr)
table(success1$success)
table(successr)
# Modifying est.ate to use the object of "randomize" class
# Just in case we use the randomize() function from the library
# For now, I've written my own
est.mate <- function(result, treat) {
mean(result[treat=="Treat"]) - mean(result[treat=="Control"])
}
# ATE for 1 randomization for testing
ateR <- est.ate(dr$views, g.randomize(count.control, count.treatment))
ateR
# This is very different from our ATE
# Repeating the expriment 10,000 times
distribution.under.sharp.null <- replicate(10000, est.ate(dr$views, g.randomize(count.control, count.treatment)))
# Average estimate for the ATE
mean(distribution.under.sharp.null)
# Again, very different from ATE
# Graph for the estimates
plot(density(distribution.under.sharp.null),
main = "Density under Sharp Null")
# Almost a normal distribution
# Histogram for the distribution
hist(distribution.under.sharp.null,
main = "Histogram under Sharp Null")
# More analysis on the distribution. Plotting the p value
par(mfrow = c(1,2))
plot(density(distribution.under.sharp.null),
main = "Density Plot of ATE")
abline(v = ate, col = "blue")
hist(distribution.under.sharp.null,
main = "Histogram of ATE",
freq = FALSE)
abline(v = ate, col = "blue")
# ate was
ate
# num of assignments that generate an estimated ATE at least as large as the actual
n <- sum(distribution.under.sharp.null >= ate)
n
# p-value
m <- mean(distribution.under.sharp.null >= ate )
m
# num of simulated random assignments with an estimated ATE that is at least as large *in absolute value* as the actual estimate of the ATE
na <- sum(abs(distribution.under.sharp.null) >= abs(ate))
na
# 2 tailed p-value
ma <- mean(abs(distribution.under.sharp.null) >= ate )
ma
library(foreign)
d2 <- read.dta("./data/Titiunik.2010.dta")
head(d2)
# Grouping the data by state
state <- group_by(d2, texas0_arkansas1)
head(state)
# Mean number of bills introduced in each state
summarize(state, count=n(), meanBills=mean(bills_introduced))
# Which state looks more productive?
# Grouping by terms
terms <- group_by(d2, term2year)
head(terms)
# Mean number of bills by terms
summarize(terms, meanBills=mean(bills_introduced))
# Which term is more effective?
# Grouping by state and term term
stateTerms <- group_by(d2, texas0_arkansas1, term2year)
head(stateTerms)
# Mean number of bills by grouping
summarize(stateTerms, meanBills=mean(bills_introduced))
# Which groupings indicate more effectiveness?
# Separating out the states
# Texas
texas <- state[which(state$texas0_arkansas1==0),]
head(texas)
# Finding out the control/treatment ratio
texas.group <- group_by(texas, term2year)
texas.summary <- summarize(texas.group, count=n())
texas.summary
# Arkansas
arkansas <- state[which(state$texas0_arkansas1==1),]
head(arkansas)
# Control/Treatment ratio
arkansas.group <- group_by(arkansas, term2year)
arkansas.summary <- summarize(arkansas.group, count=n())
arkansas.summary
# ATE estimate for Texas (based on the actual expriment)
texas_ate <- est.ate(texas$bills_introduced, texas$term2year)
texas_ate
# Considering the distribution with other random assignments
# We'll randomize the term variable. "2 years" is our treatment
# Since the grouping is binary, randomization of the entire dataset should work
# Ratio of treatment and control
texas.control <- texas.summary[which(texas.summary$term2year == 0), ]$count
texas.control
texas.treatment <- texas.summary[which(texas.summary$term2year == 1), ]$count
texas.treatment
# We'll create random sets in the same ratio
texas_r <- g.randomize(texas.control, texas.treatment)
table(texas_r)
# Repeating the expriment 10,000 times
distribution.under.sharp.null.texas <- replicate(10000, est.ate(texas$bills_introduced, g.randomize(texas.control, texas.treatment)))
# Visualizing the distribution of ATE estimates
plot(density(distribution.under.sharp.null.texas),
main = "Density under Sharp Null for Texas")
# Histogram of the ATE estimates
hist(distribution.under.sharp.null.texas,
main = "Histogram under Sharp Null for Texas")
# average estimated ate for texas
texas.ate.mean <- mean(distribution.under.sharp.null.texas)
texas.ate.mean
# Standard error is the square root of the averaged squared deviation (from the average EST estimate)
# Debug head(distribution.under.sharp.null.texas)
se_texas <- sqrt(mean((distribution.under.sharp.null.texas-mean(distribution.under.sharp.null.texas))^2))
se_texas
# ATE for arkansas
arkansas_ate <- est.ate(arkansas$bills_introduced, arkansas$term2year)
arkansas_ate
# Considering the distribution with other random assignments
# We'll randomize the term variable. "2 years" is our treatment
# Since the grouping is binary, randomization of the entire dataset should work
# Ratio of treatment and control
arkansas.control <- arkansas.summary[which(arkansas.summary$term2year == 0), ]$count
arkansas.control
arkansas.treatment <- arkansas.summary[which(arkansas.summary$term2year == 1), ]$count
arkansas.treatment
# We'll create random sets in the same ratio
arkansas_r <- g.randomize(arkansas.control, arkansas.treatment)
table(arkansas_r)
# Repeating the expriment 10,000 times
distribution.under.sharp.null.arkansas <- replicate(10000, est.ate(arkansas$bills_introduced, g.randomize(arkansas.control, arkansas.treatment)))
plot(density(distribution.under.sharp.null.arkansas),
main = "Density under Sharp Null for arkansas")
hist(distribution.under.sharp.null.arkansas,
main = "Histogram under Sharp Null for Arkansas")
# average estimated ate for arkansas
arkansas.ate.mean <- mean(distribution.under.sharp.null.arkansas)
arkansas.ate.mean
# Standard error is the square root of the averaged squared deviation (from the average EST estimate)
# head(distribution.under.sharp.null.arkansas)
se_arkansas <- sqrt(mean((distribution.under.sharp.null.arkansas-mean(distribution.under.sharp.null.arkansas))^2))
se_arkansas
# ATE for both sides combined
# Recalling previous values and then using 3.10
# ATE for texas from the experiment
texas_ate
# ATE for arkansas from the experiment
arkansas_ate
# Number of observations in texas
nrow(texas)
# Number of observations in Arkansas
nrow(arkansas)
# Total number of observations
nrow(d2)
# Combined ATE
comb_ate <- (texas_ate*nrow(texas)/nrow(d2)) + (arkansas_ate*nrow(arkansas)/nrow(d2))
comb_ate
# The summary below reveals a lof of why pooling this data would produce a biased estimate of ATE. We see that in actual calculations later
# Mean number of bills by grouping
summarize(stateTerms, meanBills=mean(bills_introduced))
# What if we pooled data for both states to calculate the ATE (Instead of the method above)?
# Value of ATE from the experiment with pooled data
pooled.ate <- est.ate(d2$bills_introduced, d2$term2year)
pooled.ate
# Recalling previous values and then putting them in 3.12 for combined SE
# Standard ATE error for Texas
se_texas
# Standard ATE error for Arkansas
se_arkansas
# Standar error for the 2 combined
se_comb <- sqrt((nrow(texas)*se_texas/nrow(d2))^2 + (nrow(arkansas)*se_arkansas/nrow(d2))^2)
se_comb
# Randomization inference to test the sharp null hypothesis (treatment effect is 0 in Texas)
# We have the results of 10,000 randomizations above in distribution.under.sharp.null.texas
head(distribution.under.sharp.null.texas)
# We also have the ate from the actual expriment
texas_ate
# Two tailed p value
p_two_texas <- mean(abs(distribution.under.sharp.null.texas) >= abs(texas_ate))
p_two_texas
# Randomization inference to test the sharp null hypothesis (treatment effect is 0 in Arkansas)
# We have the results of 10,000 randomizations above in distribution.under.sharp.null.texas
head(distribution.under.sharp.null.arkansas)
# We also have the ate from the actual expriment
arkansas_ate
# Two tailed p value
p_two_arkansas <- mean(abs(distribution.under.sharp.null.arkansas) >= abs(arkansas_ate))
p_two_arkansas
# Histogram for control and Treatment in Texas
hist(texas[which(state$term2year == 0),]$bills_introduced, xlim = c(0,140), ylim = c(0,6), col = "blue", border = F, main = "Distribution of bills_introduced under control and treatment in Texas", xlab = "Number of bills introduced in a session", density=10, angle = 135)
hist(texas[which(state$term2year == 1),]$bills_introduced, xlim = c(0, 140), ylim = c(0,6), col= "green", add = T, density = 10, angle = 45)
legend('topleft',c('Control','Treatment'),
fill = c("blue", "green"), bty = 'n',
border = NA)
# Histogram for control and treatment in Arkansas
hist(arkansas[which(state$term2year == 0),]$bills_introduced, xlim = c(0,70), ylim = c(0,10), col = "blue", border = F, main = "Distribution of bills_introduced under control and treatment in Arkansas", xlab = "Number of bills introduced in a session", density=10, angle = 135)
hist(arkansas[which(state$term2year == 1),]$bills_introduced, xlim = c(0, 70), ylim = c(0,10), col= "green", add = T, density = 10, angle = 45)
legend('topright',c('Control','Treatment'),
fill = c("blue", "green"), bty = 'n',
border = NA)
## load data
d3 <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
# Add a cluster ID to data
d3$cluster <- NA
# Assign the clusters as stated in "a"
d3$cluster <- c(1,1,2,2,3,3,4,4,5,5,6,6,7,7)
head(d3)
# pull out the cluster ID's
all_clusters <- unique(d3$cluster)
all_clusters
# Number of clusters in treatment
clusters.in.treatment <- 3
# Generic function to randomly(pseudo) pick the cluster ID's
randomize.clusters <- function(d) {
treat.clusters <- sample(x = all_clusters,
size = clusters.in.treatment,
replace = FALSE)
# returns 1 if a cluster ID in those that are picked
return(as.numeric(d$cluster %in% treat.clusters))
}
# Trying out one randomization. Now we have the data prepared for the experiment
d3$treatment <- randomize.clusters(d3)
# QUESTION & TODO More randomizations to get an estimate. For now just treating it as one expriment
# Grouping data by clusters and adding the treatment column(treatment-1, control-0 for the cluster)
clusters <- group_by(d3, cluster)
clusters.summary <- summarize(clusters, count=n(), mean_y=mean(Y), mean_d=mean(D), treatment=sum(treatment)/2 )
clusters.summary
# QUESTION When calculating variances, are we calculating among all the clusters or clusters in control and treatment respectively? My assumption is that we are doing it among all the clusters, just like in the book where the example (page 56) shows the Formulaes that take into account the entire N. Yes, we do have observations for control and treatment for each cluster( as some are in control and some are in treatment)
# Variance in y(0) clustered
variance_y <- (sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))^2))/nrow(clusters.summary)
variance_y
# Variance in y(1) clustered
variance_d <- (sum((clusters.summary$mean_d - mean(clusters.summary$mean_d))^2))/nrow(clusters.summary)
variance_d
# Covariance among y(0) and Y(1)
covariance <- sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))*(clusters.summary$mean_d - mean(clusters.summary$mean_d)))/nrow(clusters.summary)
covariance
# applying 3.22
# m is subjects in treatment (not clusters). N is count of subjects, number of clusters is 7
m <- clusters.in.treatment * 2
m
N <- nrow(d3)
N
nrow(clusters.summary)
se <- sqrt(((m*variance_y/N-m) + ((N-m)*variance_d/m) + 2*covariance)/(nrow(clusters.summary) - 1))
se
# repeating by grouping the observations in a different way
d4 <- read.csv("./data/ggChapter3.csv", stringsAsFactors = FALSE)
# Add the cluster ID to data
d4$cluster <- NA
# Assign the clusters for "a"
d4$cluster <- c(1,2,3,4,5,6,7,7,6,5,4,3,2,1)
head(d4)
# Trying out one randomization. Now we have the data prepared for the experiment
d4$treatment <- randomize.clusters(d4)
# Grouping by the cluster and creating the summary
clusters <- group_by(d4, cluster)
clusters.summary <- summarize(clusters, count=n(), mean_y=mean(Y), mean_d=mean(D), treatment=sum(treatment)/2 )
clusters.summary
# Variance in y(0) clustered
variance_y <- (sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))^2))/nrow(clusters.summary)
variance_y
# Variance in y(1) clustered
variance_d <- (sum((clusters.summary$mean_d - mean(clusters.summary$mean_d))^2))/nrow(clusters.summary)
variance_d
# Covariance among y(0) and Y(1)
covariance <- sum((clusters.summary$mean_y - mean(clusters.summary$mean_y))*(clusters.summary$mean_d - mean(clusters.summary$mean_d)))/nrow(clusters.summary - 1)
covariance
# applying 3.22
# m is subjects in treatment (not clusters). N is count of subjects, number of clusters is 7
m <- clusters.in.treatment * 2
m
N <- nrow(d4)
N
nrow(clusters.summary)
se1 <- sqrt(((m*variance_y/N-m) + ((N-m)*variance_d/m) + 2*covariance)/(nrow(clusters.summary) - 1))
se1
# Each site visitor shown the ad is exposed to $.10 of ads
# There are 1,000,000 users in one week
# Profit is $100 on every phone
# y(0) - .5% of visitors to news site buy an iphone in a week (in absence of ads)
# Users for the website in a wee
week.users <- 1000000
week.users
# Profit per phone sold
profit_per_phone <- 100
profit_per_phone
# % of visitors that buy without ads. This can be considered "control"
week.percent_users_buy <- .5
week.percent_users_buy
# Number of users that buy (irrespective of the ads)
week.number_users_buy_gen <- (.5/100)*1000000
week.number_users_buy_gen
# Total profit in a week (irrespective of the ads)
week.profit_gen <- week.number_users_buy_gen * profit_per_phone
week.profit_gen
# ad spend per user and per week for all users. This assumes all users are exposed to the ads
adspend_per_user <- .10
adspend_per_week <- adspend_per_user * week.users
adspend_per_week
# We should atleast make up the money spend on ads in a week. That means we have to sell +inc number of phones which is equivalent to +inc units of profit
inc <- adspend_per_week/profit_per_phone
inc
# so we have to sell 1000 additional phones
# Which is obs % points based on the original population eligible for ads and purchases
inc_per <- (inc/week.users)*100
inc_per
# This is breakeven effect of the advertising. Anything more would be positive ROI
# Also, we assumed that all users were shown the ad so
# Say we show the ads to only half the population. The ad spend would be half, but then we'll measure the effect on half the population also. The %'s would remain the same.
# Measured effect is .2% points
inc_measured <- .2
inc_measured
# users in control and treatment
week.users.control <- week.users/2
week.users.treatment <- week.users/2
week.users.control
week.users.treatment
# p by the formula in the question
p <- (week.users.control*week.percent_users_buy/100 + week.users.treatment*(week.percent_users_buy+inc_measured)/100)/(week.users.control + week.users.treatment)
p
# se by the formula in the question
se <- sqrt(p*(1-p)*((1/week.users.control) + (1/week.users.treatment)))
se
# tail of a 95% confidence interval
ci_tail <- se*1.96
ci_tail
# Upper and Lower limits of the confidence interval
ci1 <- .002 + ci_tail
ci1
ci2 <- .002 - ci_tail
ci2
# Width of the confidence interval
ci.wid1 <- ci1 - ci2
ci.wid1
# % size of the CI
(ci.wid1/(inc_measured/100))*100
# what if only 1% of users were in control
# users in control and treatment
week.users.control <- .01*week.users
week.users.treatment <- week.users - week.users.control
week.users.control
week.users.treatment
# p by the formula in the question
p <- (week.users.control*week.percent_users_buy/100 + week.users.treatment*(week.percent_users_buy+inc_measured)/100)/(week.users.control + week.users.treatment)
p
# se by the formula in the question
se <- sqrt(p*(1-p)*((1/week.users.control) + (1/week.users.treatment)))
se
# tail of a 95% confidence interval
ci_tail <- se*1.96
ci_tail
# Upper and Lower limits of the confidence interval
ci1 <- .002 + ci_tail
ci1
ci2 <- .002 - ci_tail
ci2
# Width of the confidence interval
ci.wid2 <- ci1 - ci2
ci.wid2
d5 <- read.csv("./data/listData.csv", stringsAsFactors = FALSE)
head(d5)
# 95% confidence interval for the difference between the treatment mean and the control mean
# Grouping the data by treatment (& control)
by.treatment <- group_by(d5, uniform_price_auction)
# Summary
by.treatment.summary <- summarize(by.treatment, count=n(), mean.bid=mean(bid), sd.bid=sd(bid))
by.treatment.summary
# Seems we have 2 equally sized groups (control and treatment) with very different average bid sizes but not a very sharp difference in standard deviation
# Getting an idea of normality (an essential for the T test)
# Extract the control and treatment separately
bid.control <- d5[which(d5$uniform_price_auction == 0),]
nrow(bid.control)
head(bid.control)
bid.treatment <- d5[which(d5$uniform_price_auction == 1),]
nrow(bid.treatment)
head(bid.treatment)
# plots
plot(density(bid.control$bid))
plot(density(bid.treatment$bid))
# T test using the analytic formula
# Mean of the 2 samples (control and test)
bid.control.mean <- mean(bid.control$bid)
bid.control.mean
bid.treatment.mean <- mean(bid.treatment$bid)
bid.treatment.mean
# Difference of the means
stat.mean.diff <- bid.treatment.mean - bid.control.mean
stat.mean.diff
# degrees of freedom
stat.df <- nrow(bid.control) + nrow(bid.treatment) -2
stat.df
# sample variances
stat.control.variance <- sum((bid.control$bid - bid.control.mean)^2)/nrow(bid.control)
stat.control.variance
stat.treatment.variance <- sum((bid.treatment$bid - bid.treatment.mean)^2)/nrow(bid.treatment)
stat.treatment.variance
# Pooled standard deviation
sdpool.small <- sqrt(((nrow(bid.control)-1)*stat.control.variance + (nrow(bid.treatment)-1)*stat.treatment.variance)/stat.df)
sdpool.small
# Pooled standard deviation with formula used for sample sizes > 30
sdpool.large <-sqrt((stat.control.variance/nrow(bid.control)) + (stat.treatment.variance/nrow(bid.treatment)))
sdpool.large
# SSQ
ssq <- (sum((bid.control$bid - bid.control.mean)^2)+ sum((bid.treatment$bid - bid.treatment.mean)^2))/(nrow(bid.control) + nrow(bid.treatment) -2)
ssq
sd <- sqrt(ssq)
sd
# We'll use the small sample 34 is very close to the "30"
# Boundaries of the confidence interval
# Taking the t value (t.95) from the t tables for 66 degress of freedom
# The value is 2.00
ci1 <- stat.mean.diff + 2.00 * sdpool.small * (sqrt(1/nrow(bid.control) + 1/nrow(bid.treatment)))
ci1
ci2 <- stat.mean.diff - 2.00 * sdpool.small * (sqrt(1/nrow(bid.control) + 1/nrow(bid.treatment)))
ci2
# t statistic
stat.t <- (bid.treatment.mean - bid.control.mean)/sqrt((ssq/nrow(bid.treatment)) + (ssq/nrow(bid.control)))
stat.t
# Verified with the R calculations below
# T test using R to verify the calculations
res <- t.test(bid.treatment$bid, bid.control$bid, var.equal = TRUE)
res
# Regression
reg <- lm(bid ~ uniform_price_auction,
data=d5)
summary(reg)
# The 95% confidence interval is the slope (coeff for "uniform_price_auction") +- 2 standard errors
# Could also be calculated using confint
confint(reg, 'uniform_price_auction', level=.95)
# Coefficients and p values from the regression
summary(reg)
# Some analysis of the data
bids.group <- group_by(d5, uniform_price_auction)
head(bids.group)
bids.summary <- summarize(bids.group, count=n())
bids.summary
# p value using the randomization inference
# ATE observed
# Using the ATE formula from a
bids.ate <- est.ate(d5$bid, d5$uniform_price_auction)
bids.ate
# Randomizing the bids (among control and treatment).
# uniform_price_auction is binary so we can just assume 0, 1 as control and treatment
# We'll keep the ratio control:treatment the same as observed (34:34)
bids.control <- bids.summary[which(bids.summary$uniform_price_auction == 0), ]$count
bids.control
bids.treatment <- bids.summary[which(bids.summary$uniform_price_auction == 1), ]$count
bids.treatment
bids.random <- g.randomize(bids.control, bids.treatment)
# DEBUG: For curiosity figuring out the est ATE of 2 of the randomization done above
# Using the modified ATE formula done for Q a
bids.rate <- est.ate(d5$bid, g.randomize(bids.control, bids.treatment))
bids.rate
bids.random <- g.randomize(bids.control, bids.treatment)
bids.rate <- est.ate(d5$bid, g.randomize(bids.control, bids.treatment))
bids.rate
# So we get quite a different value from the actual and  quite different values each time
# Repeating the experiment 5k times and getting the distribution under sharp null
# We are most interested in the differences in mean statistic
dom.distribution.under.sharp.null <- replicate(5000, est.ate(d5$bid, g.randomize(bids.control, bids.treatment)))
# Average estimate for the ATE
mean(dom.distribution.under.sharp.null)
# Again, very different from ATE
# visualizing the distribution
plot(density(dom.distribution.under.sharp.null), main = "difference of means distribution under sharp NULL")
hist(dom.distribution.under.sharp.null, main = "differences of means distribution under sharp NULL")
# More analysis on the distribution. Plotting the p value
par(mfrow = c(1,2))
plot(density(dom.distribution.under.sharp.null),
main = "Density Plot of ATE")
abline(v = bids.ate, col = "blue")
hist(dom.distribution.under.sharp.null,
main = "Histogram of ATE",
freq = FALSE)
abline(v = bids.ate, col = "blue")
bids.ate
# observations with absolute value >= ate
sum(abs(dom.distribution.under.sharp.null) >= abs(bids.ate))
# 2 tailed p-value
p2 <- mean(abs(dom.distribution.under.sharp.null) >= abs(bids.ate))
p2
# observations with value >= ate
sum(dom.distribution.under.sharp.null >= abs(bids.ate))
# single tail p value
p1 <- mean(dom.distribution.under.sharp.null >= abs(bids.ate))
p1
# p value using the analytic formulas
# p value is the area of the t distribution > t statistic (calculated above in a and b)
# We double it to get to the 2 sided value
# We'll look up a table to confirm
# Value from the table corresponding to t statistic of -2.8211 and degrees of freedom = 66
table.p1 <- .003159
table.p2 <- table.p1 * 2
table.p2
