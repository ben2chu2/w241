---
title: "Problem Set 5"
author: "Field Experiments"
output:
  pdf_document: default
  html_document: default
---

# 1. Online advertising natural experiment. 
These are simulated data (closely, although not entirely) based on a real example, adopted from Randall Lewis’ dissertation at MIT.

## Problem Setup 

Imagine Yahoo! sells homepage ads to advertisers that are quasi-randomly assigned by whether the user loads the Yahoo! homepage (www.yahoo.com) on an even or odd second of the day. More specifically, the setup is as follows. On any given week, Monday through Sunday, two ad campaigns are running on Yahoo!’s homepage. If a user goes to www.yahoo.com during an even second that week (e.g., Monday at 12:30:58pm), the ads for the advertiser are shown. But if the user goes to www.yahoo.com during an odd second during that week (e.g., Monday at 12:30:59), the ads for other products are shown. (If a user logs onto Yahoo! once on an even second and once on an odd second, they are shown the first of the campaigns the first time and the second of the campaigns the second time. Assignment is not persistent within users.)

This natural experiment allows us to use the users who log onto Yahoo! during odd seconds/the ad impressions from odd seconds as a randomized control group for users who log onto Yahoo! during even seconds/the ad impressions from even seconds. (We will assume throughout the problem there is no effect of viewing advertiser 2’s ads, from odd seconds, on purchases for advertiser 1, the product advertised on even seconds.)

Imagine you are an advertiser who has purchased advertising from Yahoo! that is subject to this randomization on two occasions. Here is a link to (fake) data on 500,000 randomly selected users who visited Yahoo!’s homepage during each of your two advertising campaigns, one you conducted for product A in March and one you conducted for product B in August (~250,000 users for each of the two experiments). Each row in the dataset corresponds to a user exposed to one of these campaigns.

```{r, message=FALSE}
library(data.table)
library(stargazer)
library(dplyr)
library(COUNT)
```
```{r}
# Libraries for robust and clustered standard errors
library(lmtest)
library(sandwich)
library(multiwayvcov)
```
### Loading the data for Q1
```{r}
d1 <- fread('./data/ps5_no1.csv')
d1
```

### A summary of the data
```{r, message=FALSE}
summary(d1)
```

The variables in the dataset are described below:

  + **product_b**: an indicator for whether the data is from your campaign for product A (in which case it is set to 0), sold beginning on March 1, or for product B, sold beginning on August 1 (in which case it is set to 1). That is, there are two experiments in this dataset, and this variable tells you which experiment the data belong to.
  + **treatment_ad_exposures_week1**: number of ad exposures for the product being advertised during the campaign. (One can also think of this variable as “number of times each user visited Yahoo! homepage on an even second during the week of the campaign.”)
  + **total_ad_exposures_week1**: number of ad exposures on the Yahoo! homepage each user had during the ad campaign, which is the sum of exposures to the “treatment ads” for the product being advertised (delivered on even seconds) and exposures to the “control ads” for unrelated products (delivered on odd seconds). (One can also think of this variable as “total number of times each user visited the Yahoo! homepage during the week of the campaign.”)
  + **week0**: For the treatment product, the revenues from each user in the week prior to the launch of the advertising campaign.
  + **week1**: For the treatment product, the revenues from each user in the week during the advertising campaign. The ad campaign ends on the last day of week 1.
  + **week2-week10**: Revenue from each user for the treatment product sold in the weeks subsequent to the campaign. The ad campaign was not active during this time.

Simplifying assumptions you should make when answering this problem:

  + The effect of treatment ad exposures on purchases is linear. That is, the first exposure has the same effect as the second exposure.
  + There is no effect of being exposed to the odd-second ads on purchases for the product being advertised on the even second.
  + Every Yahoo! user visits the Yahoo! home page at most six times a week.
  + You can assume that treatment ad exposures do not cause changes in future ad exposures.  That is, assume that getting a treatment ad at 9:00am doesn’t cause you to be more (or less) likely to visit the Yahoo home pages on an even second that afternoon, or on subsequent days.

## Questions to Answer 

## a. Run a crosstab of total_ad_exposures_week1 and treatment_ad_exposures_week1 to sanity check that the distribution of impressions looks as it should. Does it seem reasonable? Why does it look like this? (No computation required here, just a brief verbal response.)

### Checking the distribution of treatment and total expressions  

```{r, message=FALSE}
plot(d1$total_ad_exposures_week1, d1$treatment_ad_exposures_week1)
```

```{r}
table(d1$total_ad_exposures_week1, d1$treatment_ad_exposures_week1 )
```

```{r}
# Histograms for treatment_ad_exposures_week1 & total_ad_exposures_week1
hist(d1$total_ad_exposures_week1,  ylim = c(0, 160000), col = "blue", border = F, main = "total_ad_exposures_week1", xlab = "Number of ad exposures during the campaign", density=30, angle = 120)
hist(d1$treatment_ad_exposures_week1, ylim = c(0, 160000), col= "red", add=T, density = 40)
legend('topright',c('Total ad exposures in week 1','Treatment ad exposures in week 1'),
       fill = c("blue", "red"), bty = 'n',
       border = NA)

```
```{r}
# Crosstabs and other calculations
paste0("Total ad exposures: ")
table(d1$total_ad_exposures_week1)
paste0("Treatment ad exposures: ")
table(d1$treatment_ad_exposures_week1)
mte = sum(d1$total_ad_exposures_week1/nrow(d1))
paste0('Mean number of total exposures: ', mte )
mtt = sum(d1$treatment_ad_exposures_week1/nrow(d1))
paste0('Mean number of treatment exposures: ', mtt )
```
```{r}

```


**Answer: The distribution seems reasonable. The total number of ad expressions in a week are a normal distribution. The treatment expressions are a subset of total ad expressions in the week (0-6) and form a binomial distribution.The probability of getting treatment 6 times in a week is smaller than getting 1, which results in distributions like above **


## b. Your colleague proposes the code printed below to analyze this experiment: 
`lm(week1 ~ treatment_ad_exposures_week1, data)` You are suspicious. Run a placebo test with the prior week’s purchases as the outcome and report the results. Did the placebo test “succeed” or “fail”? Why do you say so?

### Running the friend's code
```{r}
lr1b = lm(week1 ~ treatment_ad_exposures_week1, data = d1)
summary(lr1b)
```
### We are suspicious. Running a placebo test. The PLACEBO broup could be folks who saw no treatment ads but saw other ads
```{r}
# Picking people with 0 treatment ad exposure but > 0 total ad exposure
d1b = d1[which(d1$treatment_ad_exposures_week1 == 0 & d1$total_ad_exposures_week1 > 0),]
paste0("Number of folks with no treatment ad exposure: ", nrow(d1b))
paste0("ratio of folks with no treatment ad exposure: ", nrow(d1b)/nrow(d1))
# head(d1b)
# summary(d1b)
```
### Running a Regression with prior week's revenue as the outcome
```{r}
lr1b1 = lm(week0 ~ total_ad_exposures_week1, data = d1b)
summary(lr1b1)
```
### Other ways to do the placebo: Regressing prior's week revenue on treatment or all ad exposures for the entire group(as there's no way to know who got the treatment in week0. Also assuming that the exposures to ads remains about the same from prior weeks)
```{r}
lr1b2 = lm(week0 ~ treatment_ad_exposures_week1, data = d1)
summary(lr1b2)
```
```{r}
lr1b3 = lm(week0 ~ total_ad_exposures_week1, data = d1)
summary(lr1b3)
```


**All forms of the test suggest an impact of ads (treatment or really any ad) on prior week's revenue. I'll call the test a "fail" as it demonstrates an impact on outcome (revenue increase) which is not caused by treatment and not captured by any other variable in the analysis**

## c. The placebo test suggests that there is something wrong with our experiment or our data analysis. We suggest looking for a problem with the data analysis. Do you see something that might be spoiling the randomness of the treatment variable? How can you improve your analysis to get rid of this problem? Why does the placebo test turn out the way it does? What one thing needs to be done to analyze the data correctly? Please provide a brief explanation of why, not just what needs to be done. (*Note: This question, and verifying that you answered it correctly in part d below, may require some thinking. If we find many people can’t figure it out, we will post another hint in a few days.*)  
**Answer: The placebo test suggest the total ad exposures (no treatment exposures) or treatment exposures having an impact on the prior week's sales.**    
**Something that may be spoiling the randomness of treatment variable may be the inclination of already heavy shoppers to see more total exposures which inturn leads to more treatment exposures**  
**TBD, how to restore randomness**  
**The Placebo test turns out this way as frequent shoppers are frequent visitors to the website, which leads higher chances of being exposed to both the treatment and non-treatment ads**  
**One way to improve the analysis would be to include the total_ad_exposures as a co-variate. This should capture the baseline effect of folks that get more exposures by being more active shoppers. Another way would be to treat the prior week's sales as a baseline and a co-variate. This also separates the impact of baseline behavior from what is caused by the exposure to treatment ads**    

## d. Implement the procedure you propose from part (c), run the placebo test for the Week 0 data again, and report the results. (This placebo test should pass; if it does not, re-evaluate your strategy before wasting time proceeding.)
```{r}
lr1d = lm(week0 ~ treatment_ad_exposures_week1 + total_ad_exposures_week1, data = d1)
summary(lr1d)
```
### The test above does not show the treatment (placebo in this case) having an impact on prior week's revenue

## e. Now estimate the causal effect of each ad exposure on purchases during the week of the campaign itself using the same technique that passed the placebo test in part (d).
```{r}
lr1e = lm(week1 ~ treatment_ad_exposures_week1 + total_ad_exposures_week1, data = d1)
summary(lr1e)
```
**Answer: Each ad exposure causes a .056(.0046) increase in the week 1 revenue.**
*This is a much smaller effect than we saw from friend's analsis in "b"*

## f. The colleague who proposed the specification in part (b) challenges your results -- they make the campaign look less successful. Write a paragraph that a layperson would understand about why your estimation strategy is superior and his/hers is biased.  
**Answer: We are trying to estimate the impact of ads on the sales of product "a" during and after the campaign. Strategy one does a simple analysis of sales on the quantity of exposure to ads for product "a". While the folks are chosen randomly for treatment (ads for product "a"), we'll still end up with a biased estimate as folks who shop more would see more ads of any kind and so more ads for treatment i.e. there chances of being in treatment are higher (because of the method by which treatment is assigned: when you hit the website on an even second). Strategy 2 fixes this by extracting the baseline effect of shopping inclination, which is reflected in the exposure to any of the ads. With the effect of "any ad" separated out, what's left is the incremental effect of the ads of product "a" only. Strategy 2 thus gives us a more accurate estimate**

## g. Estimate the causal effect of each treatment ad exposure on purchases during and after the campaign, up until week 10 (so, total purchases during weeks 1 through 10).

### Adding a variable that's sum of all purchases (week 1-10)
```{r}
d1$total_purchases = rowSums(d1[,c(5,6,7,8,9,10,11,12,13,14)]) 
head(d1)
summary(d1)
```
### Calculating the impact of Week 1 exposure on purchases for 10 weeks
```{r}
lr1g = lm(total_purchases ~ treatment_ad_exposures_week1 + total_ad_exposures_week1, data = d1)
summary(lr1g)

```
**Answer: The impact is 0.01274 (0.01758) which is not significant**

## h. Estimate the causal effect of each treatment ad exposure on purchases only after the campaign.  That is, look at total purchases only during week 2 through week 10, inclusive.

### Adding a variable for purchases from week 2-10
```{r}
d1$pur_week_2_10 = rowSums(d1[,c(6,7,8,9,10,11,12,13,14)]) 
head(d1)
summary(d1)
```
### Calculating the impact of Week 1 exposure on purchases after that week (2-10)
```{r}
lr1h = lm(pur_week_2_10 ~ treatment_ad_exposures_week1 + total_ad_exposures_week1, data = d1)
summary(lr1h)

```
**Answer: The impact is -0.04360 (0.01684) i.e a decrease of .04 for each exposure to ad in week 1**

##i. Tell a story that could plausibly explain the result from part (h).
**Answer: Lets assume that there's a limit to how much a customer or market can buy in a certain time period (say 10 weeks). This could be due to need or financial capacity. If all of that latent purchasing is accelerated due to the ad campaign in week 1, we'll see less than natural buying in later weeks. I think that's what's happening in this experiment**
The ads motivate people to buy in the week of the ads or immediately after. That demand is taken out of the system in weeks 2-10. 
**The ads are not a lost cause though. There is some positive impact when we look at the aggregate purchases from weeks 1-10. This is the purchasing that would not have happened without the ads. These are people who did not know about the product earlier or were not clear of the need.**
The +ve impact is not as big as week 1 but is also not -ve as we see from weeks 2-10.

## j. Test the hypothesis that the ads for product B are more effective, in terms of producing additional revenue in week 1 only, than are the ads for product A.
(*Hint: The easiest way to do this is to throw all of the observations into one big regression and specify that regression in such a way that it tests this hypothesis.*)
(*Hint 2: There are a couple defensible ways to answer this question that lead to different answers. Don’t stress if you think you have an approach you can defend.*)
## k. You notice that the ads for product A included celebrity endorsements. How confident would you be in concluding that celebrity endorsements increase the effectiveness of advertising at stimulating immediate purchases?

# 2. Vietnam Draft Lottery 
A [famous paper](http://sites.duke.edu/niou/files/2011/06/Angrist_lifetime-earningsmall.pdf) by Angrist exploits the randomized lottery for the Vietnam draft to estimate the effect of education on wages. (*Don’t worry about reading this article, it is just provided to satisfy your curiosity; you can answer the question below without referring to it. In fact, it may be easier for you not to, since he has some complications to deal with that the simple data we’re giving you do not.*)

## Problem Setup

Angrist’s idea is this: During the Vietnam era, draft numbers were determined randomly by birth date -- the army would literally randomly draw birthdays out of a hat, and those whose birthdays came up sooner were higher up on the list to be drafted first. For example, all young American men born on May 2 of a given year might have draft number 1 and be the first to be called up for service, followed by November 13 who would get draft number 2 and be second, etc. The higher-ranked (closer to 1) your draft number, the likelier it was you would be drafted.

We have generated a fake version of this data for your use in this project. You can find real information (here)[https://www.sss.gov/About/History-And-Records/lotter1]. While we're defining having a high draft number as falling at 80, in reality in 1970 any number lower than 195 would have been a "high" draft number, in 1971 anything lower than 125 would have been "high". 

High draft rank induced many Americans to go to college, because being a college student was an excuse to avoid the draft -- so those with higher-ranked draft numbers attempted to enroll in college for fear of being drafted, whereas those with lower-ranked draft numbers felt less pressure to enroll in college just to avoid the draft (some still attended college regardless, of course). Draft numbers therefore cause a natural experiment in education, as we now have two randomly assigned groups, with one group having higher mean levels of education, those with higher draft numbers, than another, those with lower draft numbers. (In the language of econometricians, we say the draft number is “an instrument for education,” or that draft number is an “instrumental variable.”)

Some simplifying assumptions:

+ Suppose that these data are a true random sample of IRS records and that these records measure every living American’s income without error.
+ Assume that the true effect of education on income is linear in the number of years of education obtained.
+ Assume all the data points are from Americans born in a single year and we do not need to worry about cohort effects of any kind.

### Getting the data
```{r}
d2 <- fread('./data/ps5_no2.csv')
head(d2)
```
```{r}
summary(d2)
```


## Questions to Answer

## a. Suppose that you had not run an experiment. Estimate the "effect" of each year of education on income as an observational researcher might, by just running a regression of years of education on income (in R-ish, `income ~ years_education`). What does this naive regression suggest?

### Running a naive regression
```{r}
lr2a = lm(income ~ years_education, data = d2)
summary(lr2a)
```
**Answer: The naive regressions suggests that every  year of additional education leads to a $5750.48(83.34) bump in income. This looks like a significant result (the SE is almost 1/50th of the impact)**


## b. Continue to suppose that we did not run the experiment, but that we saw the result that you noted in part (a). Tell a concrete story about why you don't believe that observational result tells you anything causal.  

**Answer: A story could be that "years of education" is not the only or the biggest cause. Other factors like "wealth in the family", "education in the family"" and the "environment" could have similar impact on both "education" and "income".An example could be a teenager raised in an academic family that also provides seed funds to startups. This teenager would be expected to go to college and can also use some of the family wealth and connections to create more**


## c. Now, let’s get to using the natural experiment. We will define “having a high-ranked draft number” as having a draft number of 80 or below (1-80; numbers 81-365, for the remaining 285 days of the year, can be considered “low-ranked”). Create a variable in your dataset indicating whether each person has a high-ranked draft number or not. Using regression, estimate the effect of having a high-ranked draft number, the dummy variable you’ve just created, on years of education obtained. Report the estimate and a correctly computed standard error. (*Hint: Pay special attention to calculating the correct standard errors here. They should match how the draft is conducted.)

## Adding the instrument variable (high_draft)  
```{r}
d2$high_draft = as.numeric(d2$draft_number <= 80)
head(d2, 40)
summary(d2)
```
### The distribution of high_draft  
```{r}
hist(d2$high_draft)
```

### Effect of high draft on the years of education    
```{r}
lr2c = lm(years_education ~ high_draft, data=d2)
summary(lr2c)
```
### Cluster SE's may be more appropriate in this situation. The draft_number is the cluster of all men in a certain birthdate range  
```{r}
lr2c$cluster.vcov <- cluster.vcov(lr2c, ~ draft_number)
coeftest(lr2c, lr2c$cluster.vcov)
# Cluster standard errors
lr2c$cluster.se <- sqrt(diag(lr2c$cluster.vcov))
paste0('Cluster SEs: ', lr2c$cluster.se)
```
**Answer: The effect of high draft number is 2.125(.03818) additional years of education**  

## d. Using linear regression, estimate the effect of having a high-ranked draft number on income. Report the estimate and the correct standard error.
```{r}
# Regression of income on high ranked draft number
lr2d = lm(income ~ high_draft, data=d2)
summary(lr2d)
lr2d$cluster.vcov <- cluster.vcov(lr2d, ~ draft_number)
coeftest(lr2d, lr2d$cluster.vcov)
# Cluster standard errors
lr2d$cluster.se <- sqrt(diag(lr2d$cluster.vcov))
paste0('Cluster SEs: ', lr2d$cluster.se)

```
**Answer: The estimate is 6637.55(511.90)**

## e. Divide the estimate from part (d) by the estimate in part (c) to estimate the effect of education on income. This is an instrumental-variables estimate, in which we are looking at the “clean” variation in both education and income that is due to the draft status, and computing the slope of the income-education line as “clean change in Y” divided by “clean change in X”. What do the results suggest?
```{r}
# Effect of education on income
eI = 6637.55/2.125
paste0("The impact of education on income: ", eI)

```
**Answer: The result suggest that each year of education has an impact of adding $3123.55 to the income. This is less than the estimate we got from the naive regression (regression of income on education without considering the instrument)**

## f. Natural experiments rely crucially on the “exclusion restriction” assumption that the instrument (here, having a high draft rank) cannot affect the outcome (here, income) in any other way except through its effect on the “endogenous variable” (here, education). Give one reason this assumption may be violated -- that is, why having a high draft rank could affect individuals’ income other than because it nudges them to attend school for longer.  
**Answer: High "draft rank" could have an impact on income aside from the nudge for higher education. This impact could be positive or negative. Lets assume that some of the folks in the high draft rank get enrolled to fight. The descipline and mental strenght from the Army days could lead to more resilience in the business setting, leading to higher income. A negative impact could also happen if going to war leads to injuy and falling out of the employment pool**

## g. Conduct a test for the presence of differential attrition by treatment condition. That is, conduct a formal test of the hypothesis that the “high-ranked draft number” treatment has no effect on whether we observe a person’s income. (Note, that an earning of $0 *actually* means they didn't earn any money.)

### We could test if the observations differ greatly between the treatment and control groups
```{r}
d2g = group_by(d2,high_draft)
summarise(d2g, count=n())
hist(d2$high_draft)
```
**We have 5 times the number of observations in control. This could be concerning**  

### Lets look at the distribution of income in control and treatment
```{r}
# Histograms 
hist(d2[which(d2$high_draft == 1) ,]$income,  ylim = c(0, 4500), col = "red", border = F, main = "Distribution of income with draft numbers", xlab = "Income", density=40, angle = 120)
hist(d2[which(d2$high_draft == 0),]$income, ylim = c(0, 4500), col= "blue", add=T, density = 10)
legend('topright',c('High draft numbers','Low draft numbers'),
       fill = c("red", "blue"), bty = 'n',
       border = NA)

```
** A skew towards lower income in both the groups. Long tail towards the higher incomes**

### Distribution of years_education in control and treatment
```{r}
# Histograms 
hist(d2[which(d2$high_draft == 1) ,]$years_education,  ylim = c(0, 4500), col = "red", border = F, main = "Distribution of year_education with draft numbers", xlab = "Years of Education", density=40, angle = 120)
hist(d2[which(d2$high_draft == 0),]$years_education, ylim = c(0, 4500), col= "blue", add=T, density = 10)
legend('topright',c('High draft numbers','Low draft numbers'),
       fill = c("red", "blue"), bty = 'n',
       border = NA)
```
**We see the skew towards higher education in our sample of High draft numbers**

**Answer: The vast difference in the number of observations is concerning. The histogram shows that control(low draft numbers) has 5 times the number of observations.It could be a sign of differential attrition (higher attrition in the "high draft" group) and can bias our estimate**
**We cannot be sure of above statement though. We have an artificial cut off at 80 to determine the draft number, which may be what's causing the different number of observations**

## h. Tell a concrete story about what could be leading to the result in part (g).
** Answer: We're assuming that the comparatively low number of observations in the treatment group are a sign of attrition. This low number or attrition could be because a large part of that group was actually drafted, went to war and was not observable at the time of the study. The attrition could be due to death, injury or physical displacement after the war**

## i. Tell a concrete story about how this differential attrition might bias our estimates.  
** Answer: The differential attrition could lead to a bial. Here's how**.
For "low draft" numbers, we are able to observe a high number of subjects. The histogram shows that they are normally distributed from low to high incomes and education. This represents almost any population. Same is not the case for "high draft".  
Many of the folks with "high draft"" numbers are actually drafted and go to war. Some of them are not observable in the employment pool after the war (Injury, retirement, Death etc). The folks from the "high draft" pool that are observable, fall in 2 categories: **
1. Those that are able to bail out of the war by enrolling in higher education
2. Those that come back more desciplined from the war, are inspired by other folks and enroll in higher education to make up for the lost time
In both 1 and 2, we may be looking at a higher motivation to study and/or earn when compared to folks that chose to go to the war. One would intuitively think that this group will continue to show a bias towards higher income.


# 3. Dinner Plates 

Suppose that researchers are concerned with the health consequences of what people eat and how much they weigh. Consider an experiment designed to measure the effect of a proposal to help people diet. Subjects are invited to a dinner and are randomly given regular-sized or slightly larger than regular sized plates. Hidden cameras record how much people eat, and the researchers find that those given larger plates eat substantially more food than those assigned small plates. 

A statistical test shows that the apparent treatment effect is far greater than one would expect by chance. The authors conclude that a minor adjustment, reducing plate size, will help people lose weight. 

## How convincing is the evidence regarding the effect of plate size of what people eat and how much they weight?  

**Answer**: The experiment measures the impact of larger plate size on the amount of food consumed. That direct impact is easy agree with. ** The leap from that conclusion to the broader and more general conclusion about "weight management ~ plate size" is not convincing due the following:**
1. The experiment does not test a "reduction" in the plate sizes. The only observations are around the "increase" from the "normal" size 
2. Weight and Weight control are fairly complex topics and are effected by a lot of personal and environmental variables. Generalizing weight management from an observation on quantity of food may not be accurate even if we tested with smaller and larger plates

## - What design and measurment improvements do you suggest? 
**Answer: The following improvements could be made: **
1. Measure plate sizes smaller and larger than the "normal"
2. Measure the pre-experiment (baseline) parameters like "amount of food", "health/disease" and activity levels" and check if they are distributed evenly among control and treatment
3. Measure the same parameters post-experiment and check if the distribution remains even and the same as before the treatment
5. Check that the outcome measures (weight) are equivalently distributed among treatment and control
6. Include some of the pre-experiment baseline measurements like "weight" and "activity" in the model, especially if its hard to get an even distribution among control and treatment

# 4. Think about Treatment Effects 

## Throughout this course we have focused on the average treatment effect. Think back to *why* we are concerned about the average treatment effect. What is the relationship between an ATE, and some individuals' potential outcomes? Make the strongest case you can for why this is *good* measure. 

**Answer**
A method to estimate ATE is to find the difference in averages between the treated and the untreated. This is useful as we mostly cannot observe a subject in treatment and control at the same time (though in some cases its possible at different times). The ATE gives us an unbiased estimate if the assignment to "treatment" and "control" is random, "treatment" is the only causal reason for the effect and the subjects do not interfere with each other. 

ATE is an estimate for the population. *An individual's potential outcome could be very different from the average (ATE)*. Its relationship with the ATE cannot be "precisely" measured. We can get estimates of the relationship by using the following:  

1. Measure the variance in treatment effect by trying pair subjects in control and treatment
2. Identify characteristics that distinguish the subjects and find the impact by varying such characteristics

While not precise on individual outcomes, this is still a *good* measure for the following reasons:
1. 




